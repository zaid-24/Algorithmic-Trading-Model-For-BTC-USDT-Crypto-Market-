{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-07 20:02:48.827865: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-07 20:02:49.004906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-07 20:02:49.004990: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-07 20:02:49.040262: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-07 20:02:49.108546: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-07 20:02:49.110192: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-07 20:02:49.963996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "sns.set()\n",
    "#tf.compat.v1.random.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>20_MA</th>\n",
       "      <th>20_std</th>\n",
       "      <th>upper_band</th>\n",
       "      <th>lower_band</th>\n",
       "      <th>...</th>\n",
       "      <th>ema_20</th>\n",
       "      <th>ema_50</th>\n",
       "      <th>ema_100</th>\n",
       "      <th>crossover_1_20</th>\n",
       "      <th>crossover_20_50</th>\n",
       "      <th>crossover_50_100</th>\n",
       "      <th>crossover_1_50</th>\n",
       "      <th>macd_line</th>\n",
       "      <th>signal_line</th>\n",
       "      <th>macd_histogram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 05:30:00</td>\n",
       "      <td>13715.65</td>\n",
       "      <td>13715.65</td>\n",
       "      <td>13400.01</td>\n",
       "      <td>13529.01</td>\n",
       "      <td>443.356199</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13529.010000</td>\n",
       "      <td>13529.010000</td>\n",
       "      <td>13529.010000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 06:30:00</td>\n",
       "      <td>13528.99</td>\n",
       "      <td>13595.89</td>\n",
       "      <td>13155.38</td>\n",
       "      <td>13203.06</td>\n",
       "      <td>383.697006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13497.967143</td>\n",
       "      <td>13516.227647</td>\n",
       "      <td>13522.555545</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-26.001709</td>\n",
       "      <td>-5.200342</td>\n",
       "      <td>-20.801368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 07:30:00</td>\n",
       "      <td>13203.00</td>\n",
       "      <td>13418.43</td>\n",
       "      <td>13200.00</td>\n",
       "      <td>13330.18</td>\n",
       "      <td>429.064572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13481.987415</td>\n",
       "      <td>13508.931661</td>\n",
       "      <td>13518.746128</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-35.936476</td>\n",
       "      <td>-11.347569</td>\n",
       "      <td>-24.588908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 08:30:00</td>\n",
       "      <td>13330.26</td>\n",
       "      <td>13611.27</td>\n",
       "      <td>13290.00</td>\n",
       "      <td>13410.03</td>\n",
       "      <td>420.087030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13475.134328</td>\n",
       "      <td>13505.053164</td>\n",
       "      <td>13516.593333</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-36.940793</td>\n",
       "      <td>-16.466214</td>\n",
       "      <td>-20.474579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 09:30:00</td>\n",
       "      <td>13434.98</td>\n",
       "      <td>13623.29</td>\n",
       "      <td>13322.15</td>\n",
       "      <td>13601.01</td>\n",
       "      <td>340.807329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13487.122487</td>\n",
       "      <td>13508.816178</td>\n",
       "      <td>13518.264950</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-22.071801</td>\n",
       "      <td>-17.587331</td>\n",
       "      <td>-4.484470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime      open      high       low     close      volume  \\\n",
       "0  2018-01-01 05:30:00  13715.65  13715.65  13400.01  13529.01  443.356199   \n",
       "1  2018-01-01 06:30:00  13528.99  13595.89  13155.38  13203.06  383.697006   \n",
       "2  2018-01-01 07:30:00  13203.00  13418.43  13200.00  13330.18  429.064572   \n",
       "3  2018-01-01 08:30:00  13330.26  13611.27  13290.00  13410.03  420.087030   \n",
       "4  2018-01-01 09:30:00  13434.98  13623.29  13322.15  13601.01  340.807329   \n",
       "\n",
       "   20_MA  20_std  upper_band  lower_band  ...        ema_20        ema_50  \\\n",
       "0    NaN     NaN         NaN         NaN  ...  13529.010000  13529.010000   \n",
       "1    NaN     NaN         NaN         NaN  ...  13497.967143  13516.227647   \n",
       "2    NaN     NaN         NaN         NaN  ...  13481.987415  13508.931661   \n",
       "3    NaN     NaN         NaN         NaN  ...  13475.134328  13505.053164   \n",
       "4    NaN     NaN         NaN         NaN  ...  13487.122487  13508.816178   \n",
       "\n",
       "        ema_100  crossover_1_20  crossover_20_50  crossover_50_100  \\\n",
       "0  13529.010000           False            False             False   \n",
       "1  13522.555545           False            False             False   \n",
       "2  13518.746128           False            False             False   \n",
       "3  13516.593333           False            False             False   \n",
       "4  13518.264950            True            False             False   \n",
       "\n",
       "   crossover_1_50  macd_line  signal_line  macd_histogram  \n",
       "0           False   0.000000     0.000000        0.000000  \n",
       "1           False -26.001709    -5.200342      -20.801368  \n",
       "2           False -35.936476   -11.347569      -24.588908  \n",
       "3           False -36.940793   -16.466214      -20.474579  \n",
       "4            True -22.071801   -17.587331       -4.484470  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax = MinMaxScaler().fit(df.iloc[:, 4:5].astype('float32')) # Close index\n",
    "df_log = minmax.transform(df.iloc[:, 4:5].astype('float32')) # Close index\n",
    "df_log = pd.DataFrame(df_log)[:1000]\n",
    "len(df_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 28), (970, 1), (30, 1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 30\n",
    "simulation_size = 10\n",
    "\n",
    "df_train = df_log.iloc[:-test_size]\n",
    "df_test = df_log.iloc[-test_size:]\n",
    "df=df[:1000]\n",
    "df.shape, df_train.shape, df_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(inputs):\n",
    "    T = inputs.shape[1]\n",
    "    repr_dim = inputs.shape[-1]\n",
    "    pos = tf.reshape(tf.range(0.0, tf.cast(T, dtype=tf.float32)), [-1, 1])\n",
    "    i = np.arange(0, repr_dim, 2, dtype=np.float32)\n",
    "    denom = np.reshape(np.power(10000.0, i / repr_dim), [1, -1])\n",
    "    enc = tf.expand_dims(tf.concat([tf.sin(pos / denom), tf.cos(pos / denom)], 1), 0)\n",
    "    return tf.tile(enc, [tf.shape(inputs)[0], 1, 1])\n",
    "\n",
    "def layer_norm(inputs, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(inputs, axes=[-1], keepdims=True)\n",
    "    normalized = (inputs - mean) / (tf.sqrt(variance + epsilon))\n",
    "    params_shape = inputs.shape[-1:]\n",
    "    gamma = tf.Variable(initial_value=tf.ones(shape=params_shape), trainable=True, name='gamma', dtype=tf.float32)\n",
    "    beta = tf.Variable(initial_value=tf.zeros(shape=params_shape), trainable=True, name='beta', dtype=tf.float32)\n",
    "    return gamma * normalized + beta\n",
    "\n",
    "def cnn_block(x, dilation_rate, pad_sz, hidden_dim, kernel_size):\n",
    "    x = layer_norm(x)\n",
    "    pad = tf.zeros([tf.shape(x)[0], pad_sz, hidden_dim])\n",
    "    x =  tf.keras.layers.Conv1D(filters=hidden_dim, kernel_size=kernel_size, dilation_rate=dilation_rate)(tf.concat([pad, x, pad], 1))\n",
    "    x = x[:, :-pad_sz, :]\n",
    "    x = tf.nn.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        kernel_size=3,\n",
    "        n_attn_heads=16,\n",
    "        dropout=0.9,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder_embedded = tf.keras.layers.Dense(size_layer)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.size_layer = size_layer\n",
    "        self.output_size = output_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_attn_heads = n_attn_heads\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_embedded = self.encoder_embedded(inputs)\n",
    "        encoder_embedded += position_encoding(encoder_embedded)\n",
    "        \n",
    "        e = tf.identity(encoder_embedded)\n",
    "        for i in range(self.num_layers): \n",
    "            dilation_rate = 2 ** i\n",
    "            pad_sz = (self.kernel_size - 1) * dilation_rate \n",
    "            encoder_embedded += cnn_block(encoder_embedded, dilation_rate, \n",
    "                                          pad_sz, self.size_layer, self.kernel_size)\n",
    "                \n",
    "        encoder_output, output_memory = encoder_embedded, encoder_embedded + e\n",
    "        g = tf.identity(encoder_embedded)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            dilation_rate = 2 ** i\n",
    "            pad_sz = (self.kernel_size - 1) * dilation_rate\n",
    "            attn_res = h = cnn_block(encoder_embedded, dilation_rate, \n",
    "                                     pad_sz, self.size_layer, self.kernel_size)\n",
    "\n",
    "            C = []\n",
    "            for j in range(self.n_attn_heads):\n",
    "                h_ = tf.keras.layers.Dense(self.size_layer // self.n_attn_heads)(h)\n",
    "                g_ = tf.keras.layers.Dense(self.size_layer // self.n_attn_heads)(g)\n",
    "                zu_ = tf.keras.layers.Dense(\n",
    "                    self.size_layer // self.n_attn_heads\n",
    "                )(encoder_output)\n",
    "                ze_ = tf.keras.layers.Dense(\n",
    "                    self.size_layer // self.n_attn_heads\n",
    "                )(output_memory)\n",
    "\n",
    "                d = tf.keras.layers.Dense(self.size_layer // self.n_attn_heads)(h_) + g_\n",
    "                dz = tf.matmul(d, tf.transpose(zu_, [0, 2, 1]))\n",
    "                a = tf.nn.softmax(dz)\n",
    "                c_ = tf.matmul(a, ze_)\n",
    "                C.append(c_)\n",
    "\n",
    "            c = tf.concat(C, 2)\n",
    "            h = tf.keras.layers.Dense(self.size_layer)(attn_res + c)\n",
    "            h = self.dropout(h, training=True)\n",
    "            encoder_embedded += h\n",
    "\n",
    "        encoder_embedded = tf.sigmoid(encoder_embedded[:, -1])\n",
    "        logits = tf.keras.layers.Dense(self.output_size)(encoder_embedded)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(real, predict):\n",
    "    real = np.array(real) + 1\n",
    "    predict = np.array(predict) + 1\n",
    "    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "    return percentage * 100\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = test_size\n",
    "epoch = 300\n",
    "dropout_rate = 0.8\n",
    "future_day = test_size\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, sess, df_train, test_size, timestamp, minmax, optimizer):\n",
    "    date_ori = pd.to_datetime(df.iloc[:, 0]).tolist()\n",
    "\n",
    "    pbar = tqdm(range(epoch), desc='train loop')\n",
    "    for i in pbar:\n",
    "        init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(df_train.iloc[k: index, :].values, axis=0)\n",
    "            batch_y = df_train.iloc[k + 1: index + 1, :].values\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(batch_x)\n",
    "                loss = tf.reduce_mean(tf.square(batch_y - logits))\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            total_loss.append(loss.numpy())\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "        pbar.set_postfix(cost=np.mean(total_loss), acc=np.mean(total_acc))\n",
    "\n",
    "    future_day = test_size\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits = model(np.expand_dims(df_train.iloc[k: k + timestamp], axis=0))\n",
    "        output_predict[k + 1: k + timestamp + 1] = out_logits.numpy()\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits = model(np.expand_dims(df_train.iloc[upper_b:], axis=0))\n",
    "        output_predict[upper_b + 1: df_train.shape[0] + 1] = out_logits.numpy()\n",
    "        future_day -= 1\n",
    "        date_ori.append(date_ori[-1] + timedelta(days=1))\n",
    "\n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i: -future_day + i]\n",
    "        out_logits = model(np.expand_dims(o, axis=0))\n",
    "        output_predict[-future_day + i] = out_logits.numpy()[-1]\n",
    "        date_ori.append(date_ori[-1] + timedelta(days=1))\n",
    "\n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "\n",
    "    return deep_future[-test_size:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loop:  16%|█▌        | 48/300 [08:36<45:43, 10.89s/it, acc=30.4, cost=0.836]"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "model = Model(learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout=dropout_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d' % (i + 1))\n",
    "    results.append(forecast(model, optimizer, df_train, test_size, timestamp, minmax, optimizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KDSH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
